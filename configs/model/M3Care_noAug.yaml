# @package _global_
defaults:
  - _self_

model_name: M3Care

PolyMNIST:
  max_epochs: 100
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  embedding_size: 128
  transformer_dim: 128
  transformer_heads: 4
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1, 1, 1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


MST:
  max_epochs: 20
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  embedding_size: 32
  transformer_dim: 32
  transformer_heads: 2
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


CelebA:
  max_epochs: 20
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  embedding_size: 32
  transformer_dim: 32
  transformer_heads: 2
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


DVM:
  max_epochs: 300
  batch_size: 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  embedding_size: 256
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


CAD:
  max_epochs: 300
  batch_size: 128  # 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  embedding_size: 256
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


Infarction:
  max_epochs: 300
  batch_size: 128 # 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  embedding_size: 256
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  num_gnn_layers: 2
  lambda_stab: 1e-8
  num_masks: 0
  augmentation_K: 

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain: