# @package _global_
defaults:
  - _self_

model_name: OnlineMAE
PolyMNIST:
  max_epochs: 100
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 128
  transformer_heads: 4
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1, 1, 1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 6
  encoder_checkpoint: /home/siyi/project/mm/result/Dynamic_project/PM31/whole_none_PolyMNIST_MultiAE_0530_162650/downstream/checkpoint_best_acc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


MST:
  max_epochs: 20
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 32
  transformer_heads: 2
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 4
  encoder_checkpoint: /home/siyi/project/mm/result/Dynamic_project/MS12/whole_none_MST_MultiAE_0530_170327/downstream/checkpoint_best_acc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


CelebA:
  max_epochs: 20
  batch_size: 256
  lr_downstream: 0.001
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 32
  transformer_heads: 2
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 4
  encoder_checkpoint: /home/siyi/project/mm/result/Dynamic_project/CA12/none_CelebA_MultiAE_0530_170459/downstream/checkpoint_best_auc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


DVM:
  max_epochs: 300
  batch_size: 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 4
  encoder_checkpoint: /vol/biomedic3/sd1523/project/mm/result/TIP_results/D20/D20_cq/D20trainable_True_value00_dvm_0126_0904/checkpoint_best_acc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


CAD:
  max_epochs: 300
  batch_size: 128 # 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 4
  encoder_checkpoint: /vol/biomedic3/sd1523/project/mm/result/TIP_results/FC7/FC7trainable_True_value00_CAD_0121_1130/checkpoint_best_auc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain:


Infarction:
  max_epochs: 300
  batch_size: 128 # 256
  lr_downstream: 0.0003
  weight_decay_downstream: 0
  # model specific
  transformer_dim: 256
  transformer_heads: 8
  transformer_layers: 2
  transformer_drop: 0.0
  num_patches_list: [1, 1]
  recon_encoder_layers: 2
  recon_decoder_layers: 2
  num_mem_tokens: 4
  encoder_checkpoint: /vol/biomedic3/sd1523/project/mm/result/TIP_results/FC7/FC7trainable_True_value00_Infarction_0125_1702/checkpoint_best_auc.ckpt

  max_epochs_pretrain:
  lr_pretrain:
  weight_decay_pretrain: